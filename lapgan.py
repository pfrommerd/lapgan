from scipy.ndimage.filters import gaussian_filter

from keras.models import Model
from keras.layers import Activation, UpSampling2D
import keras.layers

import numpy as np


# Takes a bunch of generators (z, g -> x_fake) and discriminators (x_n --> y)
# (where g is the scaled-up previous level in the gaussian pyramid and x are the laplacian differences)
# and will pair generators and discrimintors for each level in the laplacian/gaussian pyramids
# so that the resulting model has
# (z0, z1, ..., zn, real_x0,  real_x1, ..., real_xn) -->
#         (y_fake0, y_real0, y_fake1, y_real1, ..., y_faken, y_realn)
#
# real_x0 is the lowest level in the placian pyramid (i.e the actual iamge)
# while real_x1 and higher are just deltas
#
# if latent_sampling is set then the training model will not have the zn terms)
#
# the function returns as a second result also the full generator stack
# (regardless of how latent_sampling is set)
# with (z0, z1, ..., zn) --> (o_n), or (z0, z1, ..., zn, g0) --> (o_n)
def build_lapgan(generators, discriminators, latent_sampling=None, generate_base=True, name="lapgan"):
    input_z_list = [] # The noise inputs
    input_x_list = [] # The laplacian pyramid inputs
    outputs_list = [] # The discriminator outputs

    
    # We need to feed the gaussian pyramid
    # reconstruction from the laplacian into the generator
    # not the laplacian itself!
    real_g_previous = None # The real g before the current layer (None for first iteration if generate_base=True)
    real_g_next = None # The real g after the current layer
    for idx, (generator, discriminator) in enumerate(zip(generators, discriminators)):
        if real_g_previous is None and generate_base==False:
            real_g_previous = generator.inputs[1] # The previous real_g is the input to the first generator in this case
            # We need to append that to the base of the input pyramid
            input_x_list.append(real_g_previous)

        # Inputs
        real_x = discriminator.inputs[0] # Laplacian (=gaussian for idx=0)
        input_x_list.append(real_x)
            

        # real_g_previous will be none if we are generating the next real_g at the first (this) layer
        # in that scenario we will set real_g_next to be the output of this layer down below
            
        # Random noise z:
        z = generator.inputs[0]
        if latent_sampling is not None:
            z = latent_sampling(real_x) # Use the input of real_x to determine the batch size
        else:
            input_z_list.append(z)
        
        if real_g_previous is None: # We need to generate the base laplacian/gaussian
            # Here real_x is the first real_g which we are trying to generate
            # (i.e we are directly generating the smallest resolution, rather than a delta)
            # this will be the input to the next layer, so set it to real_g_next
            # it will have already been added to the input list above
            real_g_next = real_x
            yfake = Activation("linear", name=("yfake_%d" % idx))(discriminator(generator(z)))
            yreal = Activation("linear", name=("yreal_%d" % idx))(discriminator(real_g_next))
            outputs_list.append(yfake)
            outputs_list.append(yreal)
            
        else: # We have a previous laplacian/gaussian which we need to
            #forward (either supplied or generated by a previous layer)
            yfake = Activation("linear", name=("yfake_%d" % idx))(discriminator(generator([z, real_g_previous])))
            yreal = Activation("linear", name=("yreal_%d" % idx))(discriminator(real_x)) # Learn the delta
            outputs_list.append(yfake)
            outputs_list.append(yreal)

        if real_g_previous is not None:
            # Upsample real_g_previous and add real_x to get to the next layer
            real_g_next = UpSampling2D(size=(2,2))(real_g_previous)
            real_g_next = keras.layers.add([real_g_next, real_x])

        real_g_previous = real_g_next # Moving to the next layer
            
    sandwiched_model = Model(input_z_list + input_x_list, outputs_list, name=name)


    # Now do the same thing, but chain the generators together rather than going back
    # to always get the ground truth
    input_z_list = [] # The noise inputs
    input_x_list = [] # The bottommost gaussian input, if any (will be none if generate_base=True)
    outputs_list = [] # The generator outputs, at each level in the pyramid

    
    # We need to feed the gaussian pyramid
    # reconstruction from the laplacian into the generator
    # not the laplacian itself!
    g_previous = None # The real g before the current layer (None for first iteration if generate_base=True)
    g_next = None # The real g after the current layer
    for idx, (generator, discriminator) in enumerate(zip(generators, discriminators)):
        if g_previous is None and generate_base==False:
            g_previous = generator.inputs[1]
            # The previous g is the input to the first generator in this case
            # We need to append that to the base of the input pyramid
            input_x_list.append(g_previous)


        # Inputs
        
        # g_previous will be none if we are generating the next g at the first (this) layer
        # in that scenario we will set g_next to be the output of this layer down below
            
        # Random noise z:
        z = generator.inputs[0]
        input_z_list.append(z) # We will need this regardless of which layer we are on, so add it right away

        # The output of the generator, will be used to generate the next delta
        fake_x = None
        
        if g_previous is None: # We need to generate the base laplacian/gaussian
            fake_g = generator(z)
            # This is the base generation, forward to the next layer
            g_next = fake_g
        else: # We have a previous laplacian/gaussian which we need to forward (either supplied or generated by a previous layer)
            fake_x = generator([z, g_previous])

        if g_previous is not None and fake_x is not None:
            # Upsample g_previous and add fake_x to get to the next layer
            g_next = UpSampling2D(size=(2,2))(g_previous)
            g_next = keras.layers.add([g_next, fake_x])

        outputs_list.append(g_next) # Output the reconstruction after this layer
        
        g_previous = g_next # Moving to the next layer
            
    full_model = Model(input_z_list + input_x_list, outputs_list, name=name)

    return sandwiched_model, full_model


# Returns the targets for the generator and discriminator for yfake (using generator + discriminator)
# and y real (using discriminator + real input)
# Shoud return [generator_yfake0, generator_yreal0, discriminator_yfake0, discriminator_yreal0, ...]
# So this should be [1 , 0 (can be anythign really as the generator is not affected), 0, 1] repeated by the
# number of layers we have
def make_lapgan_targets(num_layers, num_samples):
    # For each player we need to give the desired
    # outputs to all layers, even if the player is not affecting
    # other layers
    generator_fake = np.ones((num_samples, 1))
    generator_real = np.zeros((num_samples, 1))
    discriminator_fake = np.zeros((num_samples, 1))
    discriminator_real = np.ones((num_samples, 1))

    # The desired outputs for all layers for a single generator and discriminator
    single_generator = [generator_fake, generator_real] * num_layers
    single_discriminator = [discriminator_fake, discriminator_real] * num_layers

    # We assume that the player order are generator/discriminator pairs
    return (single_generator + single_discriminator) * num_layers

# Will downsample by factors of 2, n times
def make_gaussian_pyramid(full_res_data, num_layers, blur_sigma):
    blur = lambda x,s: gaussian_filter(x, (0, s, s, 0))
    downsample = lambda x: x[:,::2,::2,:]

    pyramid = [full_res_data]
    for i in range(num_layers - 1):
        pyramid.append(downsample(blur(pyramid[-1], blur_sigma)))
        blur_sigma /= 2 # Decrease the blur for each layer

    return pyramid

def make_laplacian_pyramid(gaussian_pyramid):
    upsample = lambda x: x.repeat(2, 1).repeat(2,2)
    
    lap_pyramid = [gaussian_pyramid[-1]]
    for prev_layer, layer in zip(reversed(gaussian_pyramid),
                                 reversed(gaussian_pyramid[:-1])):
        diff = np.subtract(layer, upsample(prev_layer))
        lap_pyramid.append(diff)
                                 
    return lap_pyramid
    
def fix_names(outputs, names):
    if not isinstance(outputs, list):
        outputs = [outputs]
    if not isinstance(names, list):
        names = [names]
    return [Activation('linear', name=name)(output) for output, name in zip(outputs, names)]
