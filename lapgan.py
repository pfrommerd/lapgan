from scipy.ndimage.filters import gaussian_filter

from keras.models import Model
from keras.layers import Activation, UpSampling2D, Input, Lambda
import keras.layers
import keras.backend as K
from keras.layers.merge import _Merge

import utils

import os

import numpy as np


# Takes a bunch of generators (z, g -> x_fake (laplacian delta) ) and discriminators (x_n --> y)
# (where g is the scaled-up previous level in the gaussian pyramid and x are the laplacian differences)
# and will pair generators and discrimintors for each level in the laplacian/gaussian pyramids
# so that the resulting model has
# (z0, z1, ..., zn, real_g0,  real_g1, ..., real_gn) -->
#         (y_fake0, y_real0, y_fake1, y_real1, ..., y_faken, y_realn)
#
# real_gn are the images in the gaussian pyramid
#
# if latent_sampling is set then the training model will not have the zn terms)
#
# the function returns as a second result also the full generator stack
# (regardless of how latent_sampling is set)
# with (z0, z1, ..., zn) --> (o_n), or (z0, z1, ..., zn, real_g0) --> (o_n)
def build_lapgan(generators, discriminators, latent_samplers=[], supply_base=False, base_dimensions=None, name="lapgan"):
    input_z_list = [] # The noise inputs
    input_g_list = [] # The gaussian pyramid inputs
    outputs_list = [] # The discriminator outputs

    real_g_previous = None
    for idx, (generator, discriminator) in enumerate(zip(generators, discriminators)):
        if idx == 0 and supply_base==True:
            real_g_previous = Input(shape=base_dimensions)
            # The previous real_g is the input to the first generator in this case
            # We need to append that to the base of the input pyramid
            input_g_list.append(real_g_previous)

        # Inputs
        gen_out_shape = generator.output_shape
        real_g = Input(shape=(gen_out_shape[1],
                              gen_out_shape[2],
                              gen_out_shape[3])) # Gaussian pyramid input
        input_g_list.append(real_g)
            

        # real_g_previous will be none if we are generating the next real_g at the first (this) layer
        # in that scenario we will set real_g_next to be the output of this layer down below
            
        # Random noise z:
        z = generator.inputs[0]
        if len(latent_samplers) > idx:
            z = latent_samplers[idx](real_g) # Use the input of real_x to determine the batch size
        else:
            input_z_list.append(z)
        
        if real_g_previous is None: # We need to generate the base laplacian/gaussian
            # Here there is only a noise input and we get out the real_g
            # (i.e we are directly generating the smallest resolution, rather than a delta)
            yfake = Activation("linear", name=("yfake_%d" % idx))(discriminator(generator(z)))
            yreal = Activation("linear", name=("yreal_%d" % idx))(discriminator(real_g))
            outputs_list.append(yfake)
            outputs_list.append(yreal)
            
        else: # We are generating a laplacian delta from the previous real_g
            #forward (either supplied or generated by a previous layer)
            # Upsample real_g_previous
            real_g_previous = UpSampling2D(size=(2,2))(real_g_previous)

            # Get the real_x delta between real_g and real_g_previous
            real_x = _subtract([real_g, real_g_previous])

            yfake = Activation("linear", name=("yfake_%d" % idx))(discriminator(generator([z, real_g_previous])))
            yreal = Activation("linear", name=("yreal_%d" % idx))(discriminator(real_x)) # Learn the delta
            outputs_list.append(yfake)
            outputs_list.append(yreal)

        real_g_previous = real_g

        sandwiched_model = Model(inputs=input_z_list + input_g_list, outputs=outputs_list, name=name)


    # Now do the same thing, but chain the generators together rather than going back
    # to always get the ground truth
    input_z_list = [] # The noise inputs
    input_g_list = [] # The bottommost gaussian input, if any (will be none if supply_base=False)
    outputs_list = [] # The generator outputs, at each level in the pyramid

    
    # We need to feed the gaussian pyramid
    # reconstruction from the laplacian into the generator
    # not the laplacian itself!
    g_previous = None # The real g before the current layer (None for first iteration if generate_base=True)
    g_next = None # The real g after the current layer
    for idx, (generator, discriminator) in enumerate(zip(generators, discriminators)):
        if g_previous is None and supply_base==True:
            g_previous = Input(shape=base_dimensions)
            # The previous real_g is the input to the first generator in this case
            # We need to append that to the base of the input pyramid
            input_g_list.append(g_previous)

        # Inputs
        
        # g_previous will be none if we are generating the next g at the first (this) layer
        # in that scenario we will set g_next to be the output of this layer down below
            
        # Random noise z:
        z = generator.inputs[0]
        input_z_list.append(z) # We will need this regardless of which layer we are on, so add it right away

        if g_previous is None: # We need to generate the base laplacian/gaussian
            fake_g = generator(z)
            # This is the base generation, forward to the next layer
            g_next = fake_g
        else: # We have a previous laplacian/gaussian which we need to forward (either supplied or generated by a previous layer)
            # Upsample the input as the generator expects to get an upsampled version
            g_previous = UpSampling2D(size=(2,2))(g_previous)
            fake_x = generator([z, g_previous])

            # Add the delta
            g_next = keras.layers.add([g_previous, fake_x])

        outputs_list.append(g_next) # Output the reconstruction after this layer
        
        g_previous = g_next # Moving to the next layer
    full_model = Model(input_z_list + input_g_list, outputs_list, name=name)

    return sandwiched_model, full_model


# Returns the targets for the generator and discriminator for yfake (using generator + discriminator)
# and y real (using discriminator + real input)
# Shoud return [generator_yfake0, generator_yreal0, discriminator_yfake0, discriminator_yreal0, ...]
# So this should be [1 , 0 (can be anythign really as the generator is not affected), 0, 1] repeated by the
# number of layers we have
def lapgan_targets_generator(pyramid_generator, num_player_pairs):
    for batch in pyramid_generator:
        # Batch is a list of each layer, subtract a layer as 1 is the input
        num_layers = len(batch) - 1
        num_samples = batch[0].shape[0] # Number of samples in a batch
        # For each player we need to give the desired
        # outputs to all layers, even if the player is not affecting
        # other layers
        generator_fake = np.ones((num_samples, 1))
        generator_real = np.zeros((num_samples, 1))
        discriminator_fake = np.zeros((num_samples, 1))
        discriminator_real = np.ones((num_samples, 1))

        # The desired outputs for all layers for a single generator and discriminator
        single_generator = [generator_fake, generator_real] * num_layers
        single_discriminator = [discriminator_fake, discriminator_real] * num_layers

        # We assume that the player order are generator/discriminator pairs
        yield (batch, (single_generator + single_discriminator) * num_player_pairs)

# Will downsample by factors of 2, n times
def make_gaussian_pyramid(full_res_data, num_layers, blur_sigma):
    blur = lambda x,s: gaussian_filter(x, (0, s, s, 0))
    downsample = lambda x: x[:,::2,::2,:]

    pyramid = [full_res_data]
    for i in range(num_layers - 1):
        pyramid.append(downsample(blur(pyramid[-1], blur_sigma)))
        blur_sigma /= 2 # Decrease the blur for each layer

    return pyramid

# file_paths are the paths to the lower-resolution layers
def make_mmapped_gaussian_pyramid(full_res_data, layer_paths, blur_sigma, limit_samples=None):
    blur = lambda x,s: gaussian_filter(x, (0, s, s, 0))
    downsample = lambda x: x[:,::2,::2,:]

    if limit_samples is None:
        pyramid = [full_res_data]
    else:
        pyramid = [full_res_data[:limit_samples]]
        
    for layer_path in layer_paths:
        if not os.path.exists(layer_path):
            layer = downsample(blur(pyramid[-1], blur_sigma))
            np.save(layer_path, layer)

        if limit_samples is None:
            layer = np.load(layer_path, mmap_mode='r')
        else:
            layer = np.load(layer_path, mmap_mode='r')[:limit_samples]
        pyramid.append(layer)
        blur_sigma /= 2
    return pyramid

# No longer used as the pyramid is now calculated
# in-model from the gaussian pyramid to simplify training

def make_laplacian_pyramid(gaussian_pyramid):
    upsample = lambda x: x.repeat(2, 1).repeat(2,2)
    
    lap_pyramid = []
    for prev_layer, layer in zip(reversed(gaussian_pyramid),
                                 reversed(gaussian_pyramid[:-1])):
        diff = np.subtract(layer, upsample(prev_layer))
        lap_pyramid.append(diff)
                                 
    return lap_pyramid
    
def fix_names(outputs, names):
    if not isinstance(outputs, list):
        outputs = [outputs]
    if not isinstance(names, list):
        names = [names]
    return [Activation('linear', name=name)(output) for output, name in zip(outputs, names)]



class _Subtract(_Merge):
    """Layer that subtracts 2 inputs
    It takes 2 tensor inputs
    of the same shape and returns the difference
    between the first and the second (input1 - input2)
    """

    def _merge_function(self, inputs):
        if len(inputs) < 2:
            raise ValueError('Subtract layer needs at least 2 inputs')
        output = inputs[0] - inputs[1]
        return output


def _subtract(inputs, **kwargs):
    """Functional interface to the `Subtract` layer.
    # Arguments
        inputs: A list of 2 input tensors
        **kwargs: Standard layer keyword arguments.
    # Returns
        A tensor, the sum of the inputs.
    """
    return _Subtract(**kwargs)(inputs)
